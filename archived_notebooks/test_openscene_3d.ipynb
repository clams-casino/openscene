{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae9dfed-ec6f-4205-9431-5bbeb26a0d41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import open3d as o3d\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "import MinkowskiEngine as ME\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ac50d-c94c-4f0f-b1ce-3625fd6adbdd",
   "metadata": {},
   "source": [
    "## Load the 3D Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84b45ac-8432-48e1-963a-2d916cbba465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''simple config class to recreate what's done in the openscene code'''\n",
    "class ModelConfig:\n",
    "    def __init__(self, feature_2d_extractor, arch_3d):\n",
    "        self.feature_2d_extractor = feature_2d_extractor\n",
    "        self.arch_3d = arch_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8253e701-8673-4bc1-af36-447a3ce41ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoint_path = '/home/rsl_admin/openscene/checkpoints/matterport_openseg.pth'\n",
    "checkpoint_path = '/home/rsl_admin/openscene/checkpoints/scannet_openseg.pth'\n",
    "\n",
    "# checkpoint_path = '/home/rsl_admin/openscene/checkpoints/scannet_lseg.pth'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d1537f-731a-4753-be9d-f2e98f092e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'matterport' in checkpoint_path:\n",
    "    dataset = 'matterport'\n",
    "elif 'scannet' in checkpoint_path:\n",
    "    dataset = 'scannet'\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if 'openseg' in checkpoint_path:\n",
    "    embedding_space = 'openseg'\n",
    "elif 'lseg' in checkpoint_path:\n",
    "    embedding_space = 'lseg'\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc830f5e-06f9-4cd9-bd06-66830e37552e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from run.distill import get_model\n",
    "\n",
    "model_cfg = ModelConfig(\n",
    "    feature_2d_extractor=embedding_space, \n",
    "    arch_3d='MinkUNet18A',\n",
    ")\n",
    "model = get_model(model_cfg)\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a4ee00-3bac-41cb-8cd8-bc4dc9ddff6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb9889-7a64-437b-8ff3-d3a914b0787e",
   "metadata": {},
   "source": [
    "## Load CLIP text encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44fb7707-24cc-4ebb-ad9e-df1bc648c6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "if embedding_space == 'openseg':\n",
    "    clip_model = 'ViT-L/14@336px'\n",
    "elif embedding_space == 'lseg':\n",
    "    clip_model = 'ViT-B/32'\n",
    "    \n",
    "clip_pretrained, _ = clip.load(clip_model, device='cuda', jit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd371b-67f3-4c41-850a-d40ed5d9deb9",
   "metadata": {},
   "source": [
    "## Load the point cloud\n",
    "Use scan from ScanNet for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad043616-b963-4e1e-a742-6c52cbba78b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scan_ply_filepath = \"/home/rsl_admin/matterport/data/v1/scans/17DRP5sb8fy/house_segmentations/17DRP5sb8fy/house_segmentations/17DRP5sb8fy.ply\"\n",
    "\n",
    "mesh_ply = o3d.io.read_triangle_mesh(scan_ply_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae108a20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1522546, 3)\n",
      "(1522546, 3)\n"
     ]
    }
   ],
   "source": [
    "locs_np = np.asarray(mesh_ply.vertices)\n",
    "colors_np = np.asarray(mesh_ply.vertex_colors)\n",
    "\n",
    "print(locs_np.shape)\n",
    "print(colors_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb5f1e1-93f8-488a-a4c6-9db3e8dddace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([mesh_ply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a24df",
   "metadata": {},
   "source": [
    "### Function to run the 3D distill model on a input point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37d7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version of sparse_quantize in the OS code different than what's in ME\n",
    "from dataset.voxelization_utils import sparse_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64c1ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_3d_distill_model(\n",
    "    points, \n",
    "    model, \n",
    "    model_voxel_size=0.02,\n",
    "    output_precision=np.float32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Expects a point cloud of N points as an numpy array of shape (N,3)\n",
    "    Creates a voxel representation of the point cloud with M voxels\n",
    "    \n",
    "    Returns:\n",
    "        voxel_embeddings of shape (M,E), where E is the embedding dimension\n",
    "        voxel_points of shape (M,3), center points of the voxels\n",
    "        inverse_map(N,), maps voxel representation back to points\n",
    "    \"\"\"\n",
    "    \n",
    "    # voxelize the point cloud\n",
    "    coords_np = np.floor(points / model_voxel_size)\n",
    "    \n",
    "    unique_map, inverse_map = sparse_quantize(coords_np, return_index=True)\n",
    "    unique_coords = torch.Tensor(coords_np[unique_map])\n",
    "    \n",
    "    # add batch dimension to the coords\n",
    "    unique_coords_batched = ME.utils.batched_coordinates([unique_coords])\n",
    "    \n",
    "    # 3D distill model trained with no color input, uses all ones as the feature\n",
    "    feats = torch.ones(unique_coords.shape[0], 3)\n",
    "    \n",
    "    # move inputs to gpu\n",
    "    unique_coords_batched = unique_coords_batched.to('cuda')\n",
    "    feats = feats.to('cuda')\n",
    "    \n",
    "    input_st = ME.SparseTensor(features=feats, coordinates=unique_coords_batched)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(input_st)\n",
    "\n",
    "        # normalize embeddings\n",
    "        out /= out.norm(dim=-1, keepdim=True) + 1e-6\n",
    "        \n",
    "    voxel_embeddings = out.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    voxel_points = (unique_coords.cpu().numpy() * model_voxel_size) + model_voxel_size\n",
    "    \n",
    "    # use inverse_map to map embeddings to all points\n",
    "    return voxel_embeddings, voxel_points, inverse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd2178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_embeddings, voxel_points, inverse_map = run_3d_distill_model(\n",
    "    locs_np,\n",
    "    model,\n",
    "    output_precision=np.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a42031",
   "metadata": {},
   "source": [
    "### Voxelize the point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf315cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenScene uses 2cm voxelization for Scannet and Matterport\n",
    "VOXEL_SIZE = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5920da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords_np = np.floor(locs_np / VOXEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6728f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unique_map, inverse_map = sparse_quantize(coords_np, return_index=True)\n",
    "unique_coords = torch.Tensor(coords_np[unique_map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a83b5f-ad31-46ec-9ad0-d3aebacbb392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(coords_np.shape)\n",
    "# print(unique_map.shape)\n",
    "# print(inverse_map.shape)\n",
    "# print(np.all(coords_np[unique_map][inverse_map] == coords_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa9f53e6-4a16-49ad-ba9b-bd867381c2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([990767, 4])\n"
     ]
    }
   ],
   "source": [
    "# add batch dimension to the coords\n",
    "unique_coords_batched = ME.utils.batched_coordinates([unique_coords])\n",
    "\n",
    "print(unique_coords_batched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9837111-7764-46ae-86a4-57f0dc0d4b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3D distill model trained with no color input, uses all ones as the feature\n",
    "feats = torch.ones(unique_coords.shape[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "251cae3c-4426-4fb8-8cb8-fa730e227229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Move inputs to gpu\n",
    "unique_coords_batched = unique_coords_batched.to('cuda')\n",
    "feats = feats.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f6fa340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(\n",
      "  coordinates=tensor([[  0, -33,   0,   0],\n",
      "        [  0, -33,   0, 110],\n",
      "        [  0, -33,   1,   0],\n",
      "        ...,\n",
      "        [  0, -33,   1,  -1],\n",
      "        [  0, -33,  -1,   0],\n",
      "        [  0, -33,   0,  -1]], device='cuda:0', dtype=torch.int32)\n",
      "  features=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "  coordinate_map_key=coordinate map key:[1, 1, 1]\n",
      "  coordinate_manager=CoordinateMapManagerGPU_c10(\n",
      "\t[1, 1, 1, \b\b]:\tCoordinateMapGPU:990767x4\n",
      "\talgorithm=MinkowskiAlgorithm.DEFAULT\n",
      "  )\n",
      "  spatial dimension=3)\n"
     ]
    }
   ],
   "source": [
    "# Create the SparseTensor input to the model\n",
    "input_st = ME.SparseTensor(features=feats, coordinates=unique_coords_batched)\n",
    "\n",
    "print(input_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bf904-53a0-4991-b0ad-94d0534522eb",
   "metadata": {},
   "source": [
    "### Forward the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d424c62-3ba4-407b-8d0c-0fd57f575c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(input_st)\n",
    "    \n",
    "    # normalize embeddings\n",
    "    out /= out.norm(dim=-1, keepdim=True) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e09b7a2-e94c-4677-aff8-2ed1880b4b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0213,  0.0422, -0.0042,  ...,  0.0202, -0.0261, -0.0507],\n",
      "        [ 0.0095,  0.0514, -0.0341,  ...,  0.0244, -0.0036, -0.0283],\n",
      "        [ 0.0244,  0.0422, -0.0029,  ...,  0.0177, -0.0229, -0.0521],\n",
      "        ...,\n",
      "        [ 0.0246,  0.0401, -0.0056,  ...,  0.0171, -0.0248, -0.0511],\n",
      "        [ 0.0206,  0.0453, -0.0058,  ...,  0.0217, -0.0282, -0.0496],\n",
      "        [ 0.0216,  0.0410, -0.0070,  ...,  0.0164, -0.0256, -0.0502]],\n",
      "       device='cuda:0')\n",
      "torch.Size([990767, 768])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "print(out.shape)\n",
    "print(out.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8f57e75-0d8a-4353-8678-9633c49ba87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# move computed embeddings to cpu\n",
    "voxel_embeddings_np = out.cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88894a-6295-42dd-9230-e81c0b155efb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query the computed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297e318f-5f64-4c91-bf4d-313e1afb96c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_512900/490213564.py:3: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap('jet')\n"
     ]
    }
   ],
   "source": [
    "# get a color map\n",
    "import matplotlib.cm as cm\n",
    "cmap = cm.get_cmap('jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5178d0a3-b262-4521-b3ee-419fda974664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_text_embedding(query_string, encoder):\n",
    "    with torch.no_grad():\n",
    "        text = clip.tokenize([query_string]).to('cuda')\n",
    "        text_embedding = encoder.encode_text(text)\n",
    "        text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "    return text_embedding.cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22843de4-b396-4920-a185-abfd99597a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_point_scores(query_string, voxel_embeddings):\n",
    "    query_embedding = compute_text_embedding(query_string, clip_pretrained)\n",
    "    \n",
    "    # compute the similarity first for each voxel\n",
    "    similarity = voxel_embeddings @ query_embedding.T\n",
    "    \n",
    "#     # use the inverse map to get the similarity for each point\n",
    "#     similarity = similarity[inverse_map]\n",
    "    \n",
    "    return (similarity - similarity.min()) / (similarity.max() - similarity.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c1adc04-15cf-4b03-8983-9b3490530dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_single_query(query_string, point_embeddings):\n",
    "    scores = compute_point_scores(query_string, point_embeddings)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(voxel_points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(cmap(scores).reshape(-1,4)[:,:-1])\n",
    "    o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbde20f5-6798-47b2-b4cc-86d787ca10fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_single_query(\n",
    "    # 'a comfortable place',\n",
    "    # 'a desk in a scene',\n",
    "    'a place to sit',\n",
    "    voxel_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde93c4-3f0e-4de0-84e1-8681dd0fefb6",
   "metadata": {},
   "source": [
    "### Compute the scores for a list of queries for later visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1225eb1-1ddc-4cb1-b7c4-f025ea1f8316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_queries = [\n",
    "    'bed',\n",
    "    'pillow',\n",
    "    'tv',\n",
    "    'guitar',\n",
    "    'musical instrument',\n",
    "    'bicycle',\n",
    "    'sofa',\n",
    "    'oven',\n",
    "    'desk',\n",
    "    'chair',\n",
    "]\n",
    "\n",
    "prompted_queries = []\n",
    "prompted_queries += ['a photo of a {}'.format(q) for q in object_queries]\n",
    "prompted_queries += ['a {} in a scene'.format(q) for q in object_queries]\n",
    "\n",
    "object_queries += prompted_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6981a2c-3b9b-4f74-ab6f-5a381f0c0f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstract_queries = [\n",
    "    'a place to sleep',\n",
    "    'a place to cook'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80f098c6-3ee4-4702-9f14-7aa6f5733caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "room_queries = [\n",
    "    'kitchen',\n",
    "    'bathroom',\n",
    "    'bedroom',\n",
    "    'living room',\n",
    "]\n",
    "\n",
    "prompted_queries = ['a photo of a {}'.format(q) for q in room_queries]\n",
    "\n",
    "room_queries += prompted_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b40255b-4e94-456d-a506-f689ecb71e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precompute_queries = object_queries + abstract_queries + room_queries\n",
    "\n",
    "# for q in precomputed_queries:\n",
    "#     print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46d063fd-4862-4496-8201-8c7da9546409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# figure out where to save the outputs \n",
    "scan_id = scan_ply_filepath.split('/')[-1].split('.')[0]\n",
    "config_dir_name = dataset + '_' + embedding_space + '_' + scan_id\n",
    "\n",
    "save_dir = os.path.join('precomputed_queries', config_dir_name)\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efe3fa57-55a7-4e14-8551-7c2d1935c387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 40/40 [00:08<00:00,  4.84it/s]\n"
     ]
    }
   ],
   "source": [
    "skip_exist = True\n",
    "\n",
    "for q in tqdm(precompute_queries):\n",
    "    if skip_exist and q + '.npy' in os.listdir(save_dir):\n",
    "        continue\n",
    "    scores = compute_point_scores(q, voxel_embeddings_np)\n",
    "    color_scores = cmap(scores).reshape(-1,4)[:,:-1]\n",
    "    np.save(os.path.join(save_dir, q + '.npy'), color_scores)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'locs.npy'), locs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de48acd-943a-4bed-ae43-d22e7745e90d",
   "metadata": {},
   "source": [
    "## Load and visualize a precomputed query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e731f167-aca1-41a3-8a38-b86a854f3d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # visualize a single pre-computed query\n",
    "# precomputed_query = 'a photo of a living room'\n",
    "\n",
    "# color_filename = precomputed_query + '.npy'\n",
    "# assert color_filename in os.listdir(save_dir)\n",
    "\n",
    "# precomputed_locs_np = np.load(os.path.join(save_dir, 'locs.npy'))\n",
    "# precomputed_colors_np = np.load(os.path.join(save_dir, color_filename))\n",
    "\n",
    "# pcd = o3d.geometry.PointCloud()\n",
    "# pcd.points = o3d.utility.Vector3dVector(precomputed_locs_np)\n",
    "# pcd.colors = o3d.utility.Vector3dVector(precomputed_colors_np)\n",
    "# o3d.visualization.draw_geometries(\n",
    "#     [pcd], \n",
    "#     window_name=precomputed_query,\n",
    "#     width=800,\n",
    "#     height=800\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60f8d6ac-03ee-4daf-bb38-851d23a77df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize all pre-computed queries for the current configuration\n",
    "\n",
    "precomputed_locs_np = np.load(os.path.join(save_dir, 'locs.npy'))\n",
    "\n",
    "for filename in os.listdir(save_dir):\n",
    "    if filename == 'locs.npy':\n",
    "        continue\n",
    "    else:\n",
    "        precomputed_query = filename.split('.')[0]\n",
    "        precomputed_colors_np = np.load(os.path.join(save_dir, filename))\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(precomputed_locs_np)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(precomputed_colors_np)\n",
    "        o3d.visualization.draw_geometries(\n",
    "            [pcd], \n",
    "            window_name=precomputed_query,\n",
    "            width=800,\n",
    "            height=800\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56346ef2-54e4-4611-8857-d5c3b1e0b220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed3f4143-b491-45d5-a8cb-4f2eede3575e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0decaee-aa4a-4201-b860-624805b4438e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
