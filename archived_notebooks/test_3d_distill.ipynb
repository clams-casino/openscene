{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae9dfed-ec6f-4205-9431-5bbeb26a0d41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "import MinkowskiEngine as ME\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ac50d-c94c-4f0f-b1ce-3625fd6adbdd",
   "metadata": {},
   "source": [
    "## Load the 3D Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84b45ac-8432-48e1-963a-2d916cbba465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''simple config class to recreate what's done in the openscene code'''\n",
    "class ModelConfig:\n",
    "    def __init__(self, feature_2d_extractor, arch_3d):\n",
    "        self.feature_2d_extractor = feature_2d_extractor\n",
    "        self.arch_3d = arch_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8253e701-8673-4bc1-af36-447a3ce41ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoint_path = '/home/rsl_admin/openscene/checkpoints/matterport_openseg.pth'\n",
    "checkpoint_path = '/home/rsl_admin/openscene/checkpoints/scannet_openseg.pth'\n",
    "\n",
    "# checkpoint_path = '/home/rsl_admin/openscene/checkpoints/scannet_lseg.pth'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d1537f-731a-4753-be9d-f2e98f092e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'matterport' in checkpoint_path:\n",
    "    dataset = 'matterport'\n",
    "elif 'scannet' in checkpoint_path:\n",
    "    dataset = 'scannet'\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if 'openseg' in checkpoint_path:\n",
    "    embedding_space = 'openseg'\n",
    "elif 'lseg' in checkpoint_path:\n",
    "    embedding_space = 'lseg'\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc830f5e-06f9-4cd9-bd06-66830e37552e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from run.distill import get_model\n",
    "\n",
    "model_cfg = ModelConfig(\n",
    "    feature_2d_extractor=embedding_space, \n",
    "    arch_3d='MinkUNet18A',\n",
    ")\n",
    "model = get_model(model_cfg)\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a4ee00-3bac-41cb-8cd8-bc4dc9ddff6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb9889-7a64-437b-8ff3-d3a914b0787e",
   "metadata": {},
   "source": [
    "## Load CLIP text encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44fb7707-24cc-4ebb-ad9e-df1bc648c6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "if embedding_space == 'openseg':\n",
    "    clip_model = 'ViT-L/14@336px'\n",
    "elif embedding_space == 'lseg':\n",
    "    clip_model = 'ViT-B/32'\n",
    "    \n",
    "clip_pretrained, _ = clip.load(clip_model, device='cuda', jit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd371b-67f3-4c41-850a-d40ed5d9deb9",
   "metadata": {},
   "source": [
    "## Load the point cloud\n",
    "Use scan from ScanNet for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad043616-b963-4e1e-a742-6c52cbba78b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scan_ply_filepath = \"/home/rsl_admin/matterport/data/v1/scans/17DRP5sb8fy/house_segmentations/17DRP5sb8fy/house_segmentations/17DRP5sb8fy.ply\"\n",
    "\n",
    "mesh_ply = o3d.io.read_triangle_mesh(scan_ply_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae108a20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1522546, 3)\n",
      "(1522546, 3)\n"
     ]
    }
   ],
   "source": [
    "locs_np = np.asarray(mesh_ply.vertices)\n",
    "colors_np = np.asarray(mesh_ply.vertex_colors)\n",
    "\n",
    "print(locs_np.shape)\n",
    "print(colors_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb5f1e1-93f8-488a-a4c6-9db3e8dddace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([mesh_ply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a24df",
   "metadata": {},
   "source": [
    "### Function to run the 3D distill model on a input point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37d7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version of sparse_quantize in the OS code different than what's in ME\n",
    "from dataset.voxelization_utils import sparse_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64c1ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_3d_distill_model(\n",
    "    points, \n",
    "    model, \n",
    "    model_voxel_size=0.02,\n",
    "    output_precision=np.float32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Expects a point cloud of N points as an numpy array of shape (N,3)\n",
    "    Creates a voxel representation of the point cloud with M voxels\n",
    "    \n",
    "    Returns:\n",
    "        voxel_embeddings of shape (M,E), where E is the embedding dimension\n",
    "        voxel_points of shape (M,3), center points of the voxels\n",
    "        inverse_map(N,), maps voxel representation back to points\n",
    "    \"\"\"\n",
    "    \n",
    "    # voxelize the point cloud\n",
    "    coords_np = np.floor(points / model_voxel_size)\n",
    "    \n",
    "    unique_map, inverse_map = sparse_quantize(coords_np, return_index=True)\n",
    "    unique_coords = torch.Tensor(coords_np[unique_map])\n",
    "    \n",
    "    # add batch dimension to the coords\n",
    "    unique_coords_batched = ME.utils.batched_coordinates([unique_coords])\n",
    "    \n",
    "    # 3D distill model trained with no color input, uses all ones as the feature\n",
    "    feats = torch.ones(unique_coords.shape[0], 3)\n",
    "    \n",
    "    # move inputs to gpu\n",
    "    unique_coords_batched = unique_coords_batched.to('cuda')\n",
    "    feats = feats.to('cuda')\n",
    "    \n",
    "    input_st = ME.SparseTensor(features=feats, coordinates=unique_coords_batched)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(input_st)\n",
    "\n",
    "        # normalize embeddings\n",
    "        out /= out.norm(dim=-1, keepdim=True) + 1e-6\n",
    "        \n",
    "    voxel_embeddings = out.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    voxel_points = (unique_coords.cpu().numpy() * model_voxel_size) + model_voxel_size\n",
    "    \n",
    "    # use inverse_map to map embeddings to all points\n",
    "    return voxel_embeddings, voxel_points, inverse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd2178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_embeddings, voxel_points, inverse_map = run_3d_distill_model(\n",
    "    locs_np,\n",
    "    model,\n",
    "    output_precision=np.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88894a-6295-42dd-9230-e81c0b155efb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query the computed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297e318f-5f64-4c91-bf4d-313e1afb96c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_514510/490213564.py:3: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap('jet')\n"
     ]
    }
   ],
   "source": [
    "# get a color map\n",
    "import matplotlib.cm as cm\n",
    "cmap = cm.get_cmap('jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5178d0a3-b262-4521-b3ee-419fda974664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_text_embedding(query_string, encoder):\n",
    "    with torch.no_grad():\n",
    "        text = clip.tokenize([query_string]).to('cuda')\n",
    "        text_embedding = encoder.encode_text(text)\n",
    "        text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "    return text_embedding.cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22843de4-b396-4920-a185-abfd99597a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_point_scores(query_string, voxel_embeddings):\n",
    "    query_embedding = compute_text_embedding(query_string, clip_pretrained)\n",
    "    \n",
    "    # compute the similarity first for each voxel\n",
    "    similarity = voxel_embeddings @ query_embedding.T\n",
    "    \n",
    "    return (similarity - similarity.min()) / (similarity.max() - similarity.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c1adc04-15cf-4b03-8983-9b3490530dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = compute_point_scores(\n",
    "    \"a place to sit\", \n",
    "    voxel_embeddings)\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(voxel_points)\n",
    "pcd.colors = o3d.utility.Vector3dVector(cmap(scores).reshape(-1,4)[:,:-1])\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9ad46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
